{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports and datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "# Sentences\n",
    "with open('datasets/msrpar_txt.pkl', 'rb') as f:    \n",
    "    par_txt_samples, _ = pickle.load(f)\n",
    "with open('datasets/msrvid_txt.pkl', 'rb') as f:    \n",
    "    vid_txt_samples, _ = pickle.load(f)\n",
    "with open('datasets/msranswer_txt.pkl', 'rb') as f:    \n",
    "    answer_txt_samples, _ = pickle.load(f)\n",
    "with open('datasets/def2def_txt.pkl', 'rb') as f: \n",
    "    def2def_txt_samples, _ = pickle.load(f)\n",
    "# Sentences as triplets\n",
    "with open('datasets/msrpar_samples.pkl', 'rb') as f: \n",
    "    par_samples, par_scores = pickle.load(f)\n",
    "with open('datasets/msrvid_samples.pkl', 'rb') as f: \n",
    "    vid_samples, vid_scores = pickle.load(f)\n",
    "with open('datasets/msranswer_samples.pkl', 'rb') as f: \n",
    "    answer_samples, answer_scores = pickle.load(f)\n",
    "with open('datasets/def2def_samples.pkl', 'rb') as f: \n",
    "    def2def_samples, def2def_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified datasets: full (4) unified and only STS datasets (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New unified datasets: 80% of samples and 83% of triplets are from DEF2DEF\n",
    "sent_txt_samples = par_txt_samples + vid_txt_samples + answer_txt_samples + def2def_txt_samples\n",
    "samples = par_samples + vid_samples + answer_samples + def2def_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_scores]   # def2def scores 0-50 -> 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests with only STS subdatasets\n",
    "sent_txt_samples = par_txt_samples + vid_txt_samples + answer_txt_samples \n",
    "samples = par_samples + vid_samples + answer_samples\n",
    "scores = par_scores + vid_scores + answer_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with text sentences: one embedding per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity functions between two sentences and correlation with trues in a dataset of pairs\n",
    "def txt_sents_sim(ds_txt_pairs, true_scores, stop_words=True, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    sims = []\n",
    "    for pair in ds_txt_pairs:\n",
    "        v_sent0 = uv.icds_vectorize(pair[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        v_sent1 = uv.icds_vectorize(pair[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        sims.append(max(0, uv.cos_sim(v_sent0, v_sent1)))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.6030066312997344 1.5355082496804318\n",
      "Sim scores min, max, mean and std: 0.07803620708872415 5.000000000000001 3.51004306471365 1.099592301119218\n",
      "Correlation: 0.45679290202770284\n"
     ]
    }
   ],
   "source": [
    "# Main: presents correlation and additional info\n",
    "corr, sims = txt_sents_sim(sent_txt_samples, scores, stop_words=True, punct_marks=True, embed_model='w2v', mu='ratio')   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "# All datasets\n",
    "#   0.38 \n",
    "# Only STS subdatasets:\n",
    "#   0.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With text triplets: one embedding per sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get similarity between two KGs\n",
    "# Get embedding of a sentence expressed as a knowledge graph \n",
    "def txtkg_to_vector(txt_kg, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    kg_vector = np.zeros(300)\n",
    "    for txt_fact in txt_kg:\n",
    "        v_subj = uv.icds_vectorize(txt_fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * .4 #* .4 #* .8\n",
    "        v_rel = uv.icds_vectorize(txt_fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * 1. #* 1. #*.7\n",
    "        v_obj = uv.icds_vectorize(txt_fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * 2.5 #* 2.5 #* 2.5\n",
    "        v_fact = uv.icds_composition(v_subj, v_rel)\n",
    "        v_fact = uv.icds_composition(v_fact, v_obj)\n",
    "        kg_vector = uv.icds_composition(kg_vector, v_fact)\n",
    "        #kg_vector = kg_vector + v_fact\n",
    "    return(kg_vector)\n",
    "\n",
    "# Correlation with trues in a dataset of KG pairs\n",
    "def txt_kgs_sim(ds_txt_pairs, true_scores, stop_words=True, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    sims = []\n",
    "    for pair in ds_txt_pairs:\n",
    "        kg0_vector = txtkg_to_vector(pair[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        kg1_vector = txtkg_to_vector(pair[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        sims.append(max(0, uv.cos_sim(kg0_vector, kg1_vector)))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.6030066312997344 1.5355082496804318\n",
      "Sim scores min, max, mean and std: 0.0 5.000000000000002 3.1544875409962536 1.4017510271528228\n",
      "Correlation: 0.6044777287008982\n"
     ]
    }
   ],
   "source": [
    "# Main: presents correlation and additional info\n",
    "corr, sims = txt_kgs_sim(samples, scores, stop_words=True, punct_marks=True, embed_model='w2v', mu=1)   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "# All datasets\n",
    "#   0.43  No self-att 0.8 0.7 2.5\n",
    "# Only STS subdatasets:\n",
    "#   0.60  No self-att 0.4 1.0 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model: one embedding per fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives embedding of a sentence expressed as a knowledge graph \n",
    "def fact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) *.5\n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) *1.1 \n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) *2.4\n",
    "    # 2. A full composition on whole triplet\n",
    "    v_subj_rel = uv.icds_composition(v_subj, v_rel)\n",
    "    v_subj_obj = uv.icds_composition(v_subj, v_obj)\n",
    "    v_rel_obj = uv.icds_composition(v_rel, v_obj)\n",
    "    v_fact = uv.icds_composition(v_subj_rel, v_obj)    \n",
    "    #v_fact = uv.icds_composition(v_subj, v_rel_obj)    \n",
    "    #v_fact = uv.icds_composition(v_subj_obj, v_rel) \n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Returns new, context embeddings with self-attention, if requested\n",
    "def kgtxt_to_selfatt_vectors(txt_kg, stop_words=False, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio'):\n",
    "    n_facts = len(txt_kg)\n",
    "    weight_mtrx = np.empty((n_facts, n_facts))\n",
    "    v_kg = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = fact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        v_kg.append(v_fact)\n",
    "    # 1. Compute fact-wise similarity (self-attention scores)\n",
    "    for idx in range(n_facts):\n",
    "        norm0 = np.max([1.e-125, np.linalg.norm(v_kg[idx])])**1\n",
    "        for jdx in range(n_facts):\n",
    "            norm1 = np.max([1.e-125, np.linalg.norm(v_kg[jdx])])**1\n",
    "            norms_ratio = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1]))) \n",
    "            # Joint information content = IC(x,y) = IC(x) + IC(y) - <x,y>. Here ICM like dot product, with beta = 1.01  \n",
    "            #weight_mtrx[idx][jdx] = (norm0**2 + norm1**2 - v_kg[idx] @ v_kg[jdx])  #uv.cos_sim(v_kg[idx], v_kg[jdx])) \n",
    "            weight_mtrx[idx][jdx] = (norms_ratio**2) * uv.cos_sim(v_kg[idx], v_kg[jdx])   \n",
    "    # 2. Softmax/normalizing (self-att weights)\n",
    "    #norm_weight_mtrx = softmax(weight_mtrx, axis=1) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(np.tanh, axis=1, arr=weight_mtrx) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(uv.sigmoid, axis=1, arr=weight_mtrx) \n",
    "    norm_weight_mtrx = weight_mtrx \n",
    "    # 3. Context vectors (new contextual embeddings)\n",
    "    self_att_mtrx = norm_weight_mtrx @ np.array(v_kg)     \n",
    "    return(self_att_mtrx, np.array(v_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes similarity between two sentences expressed as knowledge graphs; uses self-attention if requested\n",
    "def pair_sim(kg_pair, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio'):   # kg_pair is a list of tuples of 3 strings\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    sim_mtrx = np.empty((len(kg0), len(kg1)))\n",
    "    if self_att:\n",
    "        self_att_mtrx0, _ = kgtxt_to_selfatt_vectors(kg0, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu)\n",
    "        self_att_mtrx1, _ = kgtxt_to_selfatt_vectors(kg1, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu)\n",
    "        for idx in range(len(kg0)):\n",
    "            for jdx in range(len(kg1)):\n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx])) \n",
    "    else:          \n",
    "        for idx, fact0 in enumerate(kg0):\n",
    "            fact0_vector, _, _, _ = fact_to_vector(fact0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "            for jdx, fact1 in enumerate(kg1):\n",
    "                fact1_vector, _, _, _ = fact_to_vector(fact1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(fact0_vector, fact1_vector))\n",
    "    #sents_sim = (np.mean(sim_mtrx)) \n",
    "    sents_sim = (uv.bidir_avgmax_sim(sim_mtrx, stdst='mean'))  \n",
    "    #sents_sim = uv.bertscore(sim_mtrx) \n",
    "    return(sents_sim)\n",
    "\n",
    "# Receives a dataset, calls necessary functions, and returns a list of correlations between true and predicted similarities\n",
    "def ds_sents_sim(ds, true_scores, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio'):\n",
    "    sims = []\n",
    "    for pair in ds:\n",
    "        sims.append(pair_sim(pair, self_att=self_att, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.6030066312997344 1.5355082496804318\n",
      "Sim scores min, max, mean and std: 0.0 5.0 3.150165341045448 1.2528576225134695\n",
      "Correlation: 0.6556616326365252\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a labeled dataset and gives additional info \n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=False,  \n",
    "                        stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1)   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "# All datasets\n",
    "#   0.42 No self-att 1.0 1.0 2.1 \n",
    "#   0.45 Self-att    0.8 0.8 2.3 s_words=True mu=ratio\n",
    "# Only STS subdatasets\n",
    "#   0.66 No self-att 0.5 1.1 2.4 s_words=False\n",
    "#   0.65 Self_att    0.4 1.1 2.6 s_words=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file: ok 20250318"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
